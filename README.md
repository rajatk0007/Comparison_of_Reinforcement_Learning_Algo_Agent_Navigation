# Agent Navigation using reinforcement algorithms (comparative study)

The objective of this project is to understand different reinforcement learning algorithms for the use case of agent navigation.



## Preface

To make this comparative study and also understand these algorithms we have chosen to take a 2-dimensional input by feeding a random matrix/tensor with obstacles.
The algorithms we have used are:
<br>
<b>1) Q-Learning </b>

<b>2)SARSA</b>

<b>3) Deep Q-networks</b>

We have also made a comparative study to understand the benefits of each of these algorithms and their potential use case scenerios.

## Abstract
Many problems in various fields are solved by proposing path planning. Every decision in path planning algorithms is selected according to the available information in the current state and used criteria. In terms of optimization, the ideal path must be the shortest distance and collision- free and spend the shortest time to reach the goal state. It is a computational problem to find a sequence of valid configurations that moves the object from the source to the destination. The learning process takes several iterations and takes a long time to find the final path. Reinforcement learning has been widely used as a learning mechanism for an artificial life system. We tend to build a project that will compare path planning algorithms like Q- learning, SARSA, DQL (Path planning via deep reinforcement learning is an end-to-end method) in terms of optimality and time taken. We will also visualize the path planned by each one of them using simple graphics.

## INTRODUCTION
In Reinforcement Learning (RL), agents are trained on a reward and punishment mechanism. The agent is rewarded for correct moves and punished for the wrong ones. In doing so, the agent tries to minimize wrong moves and maximize the right ones. Reinforcement learning deals with sequential decision-making. The agent observes the environment, picks an action, performs the action, observes the environment, picks an action, etc. Planning is coming up with future actions without performing them. This project aims at comparing the reinforcement learning algorithms based on time, optimality, etc. The algorithms under discussion in this project are:
1) Q-Learning
2) SARSA [1]
3) Deep Q- Learning

<b>Q-Learning</b>- Q-Learning algorithm is famous for the principle of reward and penalties. It is a kind of mapping that represents the relationship between states and action. It is a model-free, off- policy /reinforcement learning that will find the best course of action, given the current state of the agent. Depending on where the agent is in the environment, it will decide the next action to be taken. Model-free means that the agent uses predictions of the environment’s expected response to move forward. It does not use the reward system to learn, but rather, trial and error. The goal is to learn iteratively the optimal Q-value function using the Bellman Optimality Equation. To do so, we store all the Q-values in a table that we will update at each time step using the Q-Learning iteration:

<b>Q[state, action] = Q[state, action] + lr * (reward + gamma * np.max(Q[new_state, :]) — Q[state, action])</b>

<b>SARSA</b>- State-action-reward-state-action (SARSA) is one of the well-known on-policy algorithms. A SARSA agent interacts with the environment and updates the policy based on actions taken. The Q value for a state-action is updated by an error, adjusted by the learning rate alpha. It has a certain degree of progress in convergence.
SARSA is an algorithm that updates the Q-table with the (S, A, R, S’) samples generated by the current policy. (S’, A’) is the next state and next action in transition samples. After reaching S’, it will take action A’ and use Q (S’, A’) to update the Q-value.
ε-greedy for exploration in algorithm means with ε probability, the agent will take action randomly. This method is used to increase the exploration because, without it, the agent may be stuck in a local optimal.

<b>Q[state, action] = Q[state, action] + lr * (reward + gamma * Q[new_state, new_action]) — Q[state, action])</b>

<b>Deep Q- Learning</b>- In deep Q-Learning, we combine Q-Learning with a neural network to break the chain and find the optimal Q-value function. In the algorithm of deep Q-Learning, we use states as input and the optimal Q-value of all possible actions as the output. All past experiences are stored in memory and the future action is defined by the output of the Q-Network.
It uses a deep neural network to approximate the values. This approximation of values does not hurt as long as the relative importance is preserved. The basic working step for Deep Q-Learning is that the initial state is fed into the neural network and it returns the Q-value of all possible actions as an output.

<img width="603" alt="Screenshot 2022-09-12 at 12 51 07 PM" src="https://user-images.githubusercontent.com/45784014/189595586-890e67a3-9f94-445b-802d-8fbe671cba02.png">

We have done a comparative study of the different algorithms and reached to a conclusion. This can be found in the result pdf folder above. Alternatively, the .ipynb files can be run on your local machine or a cloud service to get similar results if you'd like that. The input and output examples are given in the .ipynb files for each algorithm. Do check them out for some visuals.

=) =) =) =)
